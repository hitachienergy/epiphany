---

- name: Setup group
  group:
    name: "{{ specification.kafka_var.group }}"
    system: yes

- name: Setup user
  user:
    name: "{{ specification.kafka_var.user }}"
    system: yes
    group: "{{ specification.kafka_var.group }}"
    shell: "/usr/sbin/nologin"


- name: Install Java package
  package:
    name: "java-1.8.0-openjdk-headless"
    state: present
  when: ansible_os_family == "RedHat"

- name: Install Java package
  package:
    name: "openjdk-8-jre-headless"
    state: present
  when: ansible_os_family == "Debian"

- name: Set Kafka file name to install
  set_fact:
    kafka_file_name: "{{ specification.kafka_var.file_name }}"

- name: Download Kafka binaries
  include_role:
    name: download
    tasks_from: download_file
  vars:
    file_name: "{{ kafka_file_name }}"

# - name: Check for Kafka package
#   stat:
#     path: "/tmp/kafka_{{ specification.kafka_var.scala.version }}-{{ specification.kafka_var.version }}.tgz"
#   register: kafka_check
  
# - name: Fetch Kafka binary package
#   get_url:
#     url: "https://archive.apache.org/dist/kafka/{{ specification.kafka_var.version }}/kafka_{{ specification.kafka_var.scala.version }}-{{ specification.kafka_var.version }}.tgz"
#     dest: "/tmp/kafka_{{ specification.kafka_var.scala.version }}-{{ specification.kafka_var.version }}.tgz"
#     validate_certs: "{{ validate_certs | bool }}"
#   when: not kafka_check.stat.exists

# - name: Get sha512 sum of archive
#   stat:
#     path: "/tmp/kafka_{{ specification.kafka_var.scala.version }}-{{ specification.kafka_var.version }}.tgz"
#     checksum_algorithm: sha512
#     get_checksum: yes
#   register: kafka_download_stat

# - name: Display sha of archive
#   debug:
#     msg: "Kafka SHA512: {{ kafka_download_stat.stat.checksum }}"

# - name: Verify sha512 of archive before installation
#   fail:
#     msg: "File checksum is not correct."
#   when: kafka_download_stat.stat.checksum != specification.kafka_var.sha

- name: Add Kafka's bin dir to the PATH
  copy:
    content: "export PATH=$PATH:/opt/kafka/bin"
    dest: "/etc/profile.d/kafka_path.sh"
    mode: 0755

- name: Check for Kafka package
  stat:
    path: /opt/kafka_{{ specification.kafka_var.scala.version }}-{{ specification.kafka_var.version }}/bin/kafka-server-start.sh
  register: kafka_package

- name: Uncompress the Kafka tar
  unarchive:
    remote_src: yes
    creates: /opt/kafka_{{ specification.kafka_var.scala.version }}-{{ specification.kafka_var.version }}
    src: "{{ download_directory }}/{{ kafka_file_name }}"
    dest: /opt
  when: not kafka_package.stat.exists

- name: Change ownership on kafka directory.
  file:
    path: /opt/kafka_{{ specification.kafka_var.scala.version }}-{{ specification.kafka_var.version }}
    state: directory
    owner: kafka
    group: kafka

- name: Link /opt/kafka to the right version
  file:
    dest: /opt/kafka
    state: link
    src: /opt/kafka_{{ specification.kafka_var.scala.version }}-{{ specification.kafka_var.version }}

- name: Create systemd config
  template:
    dest: /etc/systemd/system/kafka.service
    owner: root
    group: root
    mode: 0644
    src: kafka.service.j2
  notify:
    - restart kafka

- name: Reload daemon
  command: systemctl daemon-reload

- name: Create data_dir
  file:
    path: "{{ specification.kafka_var.data_dir }}"
    state: directory
    owner: "{{ specification.kafka_var.user }}"
    group: "{{ specification.kafka_var.group }}"
    mode: 0755

- name: Remove lost+found in the datadir
  file:
    path: "{{ specification.kafka_var.data_dir }}/lost+found"
    state: absent

- name: Create log_dir
  file:
    path: "{{ specification.kafka_var.log_dir }}"
    state: directory
    owner: "{{ specification.kafka_var.user }}"
    group: "{{ specification.kafka_var.group }}"
    mode: 0755

- name: Create /etc/kafka directory
  file:
    path: /etc/kafka
    state: directory
    owner: "{{ specification.kafka_var.user }}"
    group: "{{ specification.kafka_var.group }}"

# - name: link conf_dir to /opt/kafka/config
#   file: dest=/etc/kafka owner=kafka group=kafka state=link src=/opt/kafka/config

# Setup log4j.properties
- name: Create log4j.properties
  file:
    dest: "{{ specification.kafka_var.conf_dir }}/log4j.properties"
    owner: "{{ specification.kafka_var.user }}"
    group: "{{ specification.kafka_var.group }}"
    mode: 0644
    src: log4j.properties
  notify:
    - restart kafka

# Setup server.properties
- name: Create server.properties
  template:
    dest: "{{ specification.kafka_var.conf_dir }}/server.properties"
    owner: "{{ specification.kafka_var.user }}"
    group: "{{ specification.kafka_var.group }}"
    # Was 0640
    mode: 0644
    src: server.properties.j2
  notify:
    - restart kafka

- name: Copy logrotate config
  template:
    dest: /etc/logrotate.d/kafka
    owner: root
    group: root
    mode: 0644
    src: logrotate.conf.j2

- name: configure system settings, file descriptors and number of threads for kafka
  pam_limits:
    domain: "{{ specification.kafka_var.user }}"
    limit_type: "{{item.limit_type}}"
    limit_item: "{{item.limit_item}}"
    value: "{{item.value}}"
  with_items:
    - { limit_type: '-', limit_item: 'nofile', value: 128000 }
    - { limit_type: '-', limit_item: 'nproc', value: 128000 }
    - { limit_type: 'soft', limit_item: 'memlock', value: unlimited }
    - { limit_type: 'hard', limit_item: 'memlock', value: unlimited }
    
- name: reload settings from all system configuration files
  shell: sysctl --system

# SASL Setup
# - name: copy SASL config file
#   template: src=kafka_server_jaas.conf.j2 dest={{kafka_var.conf_dir}}/kafka_server_jaas.conf owner={{kafka_var.user}} group={{kafka_var.group}} mode=640
#   when: kafka_sasl_enabled
