---
# Based on https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/upgrading_from_rhel_7_to_rhel_8/index
# and partially on https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/redhat/redhat-in-place-upgrade

# Requirements:
# - Leapp metadata archive from the Red Hat portal which cannot be shared publicly.
# - Epiphany manifest with credentials (for 'aws' provider only).
# - System attached to RHUI repositories or Red Hat subscription (for 'non_cloud' provider only).

# Usage:
# ansible-playbook -e leapp_archive=/absolute/path/leapp-data16.tar.gz -e epiphany_manifest=/shared/build/aws/manifest.yml

# Note:
# For AWS playbook:
# - creates/overwrites with backup '/root/.aws/credentials' file locally
# - suspends ReplaceUnhealthy process for auto scaling groups
# - disables auto-recovery for all instances

# Limitations:
# - Ansible connection as root is not supported (PermitRootLogin)


- name: In-place RHEL release upgrade
  hosts: "{{ target | default('all') }}"
  become: true
  vars:
    versions:
      required: {major: '7', full: '7.9'}  # minimal version from which upgrade is supported
      target: {major: '8', full: '8.4'}

    leapp_dependencies:
      packages:
        aws:
          - leapp-rhui-aws
          - rh-amazon-rhui-client
        azure:
          - leapp-rhui-azure
          - rhui-azure-rhel7
        non_cloud:
          - leapp-upgrade
      repos:
        aws:
          - name: rhui-client-config-server-7
            repo_file: /etc/yum.repos.d/redhat-rhui-client-config.repo
          - name: rhel-7-server-rhui-extras-rpms
            repo_file: /etc/yum.repos.d/redhat-rhui.repo
        azure:
          - name: rhui-microsoft-azure-rhel7
            repo_file: /etc/yum.repos.d/rh-cloud.repo
          - name: rhui-rhel-7-server-rhui-extras-rpms
            repo_file: /etc/yum.repos.d/rh-cloud.repo
        non_cloud:
          - name: rhel-7-server-extras-rpms
            repo_file: /etc/yum.repos.d/redhat.repo

    reboot:
      connect_timeout: 5  # maximum seconds to wait for a successful connection to the managed hosts before next trial
      update_timeout: 500
      upgrade_timeout: 3600  # long wait required by Leapp utility service

  tasks:
    - name: End the play for already upgraded hosts
      meta: end_host
      when: ansible_distribution_version is version(versions.target.full, '>=')

    ### PRE-UPGRADE ###

    - name: Assert required major version
      assert:
        that: ansible_distribution_major_version == versions.required.major
        fail_msg: Upgrade from current major version is not supported
        quiet: true

    - name: Validate Leapp metadata file
      block:
        - name: Assert leapp_archive variable
          assert:
            that:
              - leapp_archive is defined
              - leapp_archive | length > 0
            quiet: true

        - name: Check if Leapp metadata file exists
          stat:
            path: "{{ leapp_archive }}"
            get_attributes: false
            get_checksum: false
            get_mime: false
          delegate_to: localhost
          register: stat_leapp_archive

        - name: Assert Leapp metadata file exists
          assert:
            that: stat_leapp_archive.stat.exists
            fail_msg: "File not found: {{ leapp_archive }}"
            quiet: true

    - name: Detect and set provider type
      block:
        - name: Get information on repository files
          find:
            path: /etc/yum.repos.d
            patterns: '*.repo'
          register: find_repo_files

        - name: Set provider
          set_fact:
            provider: >-
              {{ 'aws' if _repo_file_paths | select('search', leapp_dependencies.repos.aws.0.repo_file) else
                 'azure' if _repo_file_paths | select('search', leapp_dependencies.repos.azure.0.repo_file) else
                 'non_cloud' }}
          vars:
            _repo_file_paths: "{{ find_repo_files.files | map(attribute='path') }}"

        - name: Print detected provider
          debug:
            var: provider

    - name: Update repository certificates
      when: provider == "azure"
      yum:
        enablerepo: rhui-microsoft-azure-rhel7
        disablerepo: "*"
        state: latest
        update_only: true
        name: "*"

    - name: Register SELinux state
      set_fact:
        pre_upgrade_selinux_facts: "{{ ansible_facts.selinux }}"

    - name: Register enabled repositories
      command: yum repolist --quiet
      register: pre_upgrade_enabled_repositories
      changed_when: false

    # Disable legacy containerd plugin to avoid error (modprobe: FATAL: Module aufs not found)

    - name: Check if /etc/containerd/config.toml file exists
      stat:
        path: /etc/containerd/config.toml
        get_attributes: false
        get_checksum: false
        get_mime: false
      register: stat_containerd_config

    - name: Disable aufs plugin
      when: stat_containerd_config.stat.exists
      block:
        - name: Get disabled_plugins
          command: grep -oPz '(?s)^disabled_plugins\s*=\s*\[.*?\]' /etc/containerd/config.toml  # TOML allows line breaks inside arrays
          changed_when: false
          register: grep_disabled_plugins
          failed_when: grep_disabled_plugins.rc > 1

        - name: Set plugins to be disabled
          set_fact:
            plugins_to_disable: "{{ _disabled_plugins | union(['aufs']) }}"
          vars:
            _disabled_plugins: >-
              {{ (grep_disabled_plugins.stdout | regex_replace('\s*=', ':') | from_yaml).disabled_plugins | default('[]') }}

        - name: Disable aufs plugin (update array)
          replace:  # handles multi-line array
            path: /etc/containerd/config.toml
            regexp: ^disabled_plugins\s*=\s*\[[^\]]*?\]
            replace: disabled_plugins = {{ plugins_to_disable | string }}
            backup: true
          register: update_containerd_config_option

        - name: Disable aufs plugin (add array)
          lineinfile:
            path: /etc/containerd/config.toml
            line: disabled_plugins = {{ plugins_to_disable | string }}
            backup: true
          register: add_containerd_config_option
          when: grep_disabled_plugins.rc == 1

        - name: Restart containerd service
          systemd:
            name: containerd.service
            state: restarted
          when: update_containerd_config_option.changed
             or add_containerd_config_option.changed

    # AWS: Disable instance auto-recovery

    - name: Suspend ReplaceUnhealthy process for auto scaling groups and disable auto-recovery
      when: provider == 'aws'
      run_once: true
      delegate_to: localhost
      block:
        - name: Load Epiphany manifest
          slurp:
            src: "{{ epiphany_manifest }}"
          register: slurp_epiphany_manifest

        - name: Set cloud properties
          vars:
            _cluster_doc: >-
              {{ slurp_epiphany_manifest['content'] | b64decode | from_yaml_all
                                                    | selectattr('kind', '==', 'epiphany-cluster')
                                                    | first }}
          block:
            - name: Set cloud facts
              set_fact:
                aws_config_dir: "{{ '~root' | expanduser }}/.aws"
                aws_region: "{{ _cluster_doc.specification.cloud.region }}"
                cluster_name: "{{ _cluster_doc.specification.name }}"
                cluster_full_name: "{{ _cluster_doc.specification.prefix }}-{{ _cluster_doc.specification.name }}"

            - name: Create AWS configuration directory
              file:
                path: "{{ aws_config_dir }}"
                state: directory
                mode: u=rwx,go=rx

            - name: Check if AWS credentials file exists
              stat:
                path: "{{ aws_config_dir }}/{{ item }}"
                get_attributes: false
                get_checksum: false
                get_mime: false
              register: stat_aws_credentials_file
              loop:
                - credentials
                - credentials.rhel-7-upgrade.bak

            - name: Back up AWS credentials file
              when:
                - stat_aws_credentials_file.results[0].stat.exists
                - not stat_aws_credentials_file.results[1].stat.exists
              copy:
                src: "{{ aws_config_dir }}/credentials"
                dest: "{{ aws_config_dir }}/credentials.rhel-7-upgrade.bak"
                remote_src: true
                mode: preserve
              no_log: true

            - name: Create AWS credentials file
              copy:
                dest: "{{ aws_config_dir }}/credentials"
                content: |
                  [default]
                  aws_access_key_id = {{ _cluster_doc.specification.cloud.credentials.key }}
                  aws_secret_access_key = {{ _cluster_doc.specification.cloud.credentials.secret }}
                mode: u=rw,go=
              no_log: true

        - name: Find auto scaling groups
          community.aws.ec2_asg_info:
            name: "{{ cluster_full_name }}"
            region: "{{ aws_region }}"
          register: cluster_asgs

        - name: Reconfigure ASGs to suspend HealthCheck and ReplaceUnhealthy processes
          when: cluster_asgs.results | count > 0
          block:
            - name: Set facts on ASGs
              set_fact:
                asg_facts: "{{ cluster_asgs.results | json_query(_query) }}"
              vars:
                _query: '[].{auto_scaling_group_name: auto_scaling_group_name, instances: instances, suspended_processes: suspended_processes}'

            - name: Set path to file with original configuration of ASGs
              set_fact:
                asg_config_file_path: "{{ playbook_dir }}/{{ cluster_full_name }}-asg-config.yml"

            - name: Check if backup of original configuration of ASGs exists
              stat:
                path: "{{ asg_config_file_path }}"
                get_attributes: false
                get_checksum: false
                get_mime: false
              register: stat_asg_config_yml

            - name: Back up configuration of auto scaling groups
              when: not stat_asg_config_yml.stat.exists
              become: false
              copy:
                dest: "{{ asg_config_file_path }}"
                mode: u=rw,g=r,o=
                content: |
                  # This file is managed by Ansible and is needed to restore original configuration. DO NOT EDIT.
                  {{ asg_facts | to_nice_yaml(indent=2) }}

            - name: Suspend HealthCheck and ReplaceUnhealthy processes
              community.aws.ec2_asg:
                name: "{{ item.auto_scaling_group_name }}"
                suspend_processes: "{{ item.suspended_processes | union(['HealthCheck', 'ReplaceUnhealthy']) }}"
                region: "{{ aws_region }}"
              loop_control:
                label: "{{ item.auto_scaling_group_name }}"
              loop: >-
                {{ cluster_asgs.results }}

        # Ansible modules don't support `ec2 modify-instance-maintenance-options` command so we use AWS cli
        - name: Ensure pip3
          block:
            - name: Check if pip3 is present
              command: pip3 --version
              register: check_pip3
              changed_when: false
              failed_when: false

            - name: Install pip3
              command: python3 -m ensurepip
              when: check_pip3.rc != 0

        - name: Install AWS cli
          pip:
            name: awscli
          register: install_awscli

        - name: Find cluster instances
          community.aws.ec2_instance_info:
            filters:
              "tag:cluster_name": "{{ cluster_name }}"
              instance-state-name: ['running']
            region: "{{ aws_region }}"
          register: cluster_instances

        - name: Disable auto-recovery for all instances
          command: >-
            aws ec2 modify-instance-maintenance-options
              --instance-id {{ item }} --auto-recovery disabled --region {{ aws_region }}
          loop: >-
            {{ cluster_instances.instances | map(attribute='instance_id') }}

    - &UPDATE_ALL_PACKAGES
      name: Update all packages in current major version
      yum:
        update_cache: true
        name: "*"
        state: latest  # noqa: package-latest

    - name: Check if reboot is needed
      command: needs-restarting --reboothint  # exit code 1 means reboot is required
      changed_when: false
      register: needs_restarting
      failed_when: needs_restarting.rc > 1

    - name: Reboot system after update  # to load kernel from latest minor version if any
      when: needs_restarting.rc == 1
      reboot:
        msg: Reboot initiated by Ansible due to update
        connect_timeout: "{{ reboot.connect_timeout }}"
        reboot_timeout: "{{ reboot.update_timeout }}"
      vars:
        # disable connection sharing to avoid random warning on stderr: Failed to reset connection
        ansible_control_path: none

    - &REFRESH_ANSIBLE_FACTS
      name: Refresh Ansible facts
      setup:
        gather_subset: min

    - name: Assert latest minor version
      assert:
        that: ansible_distribution_version is version(versions.required.full, '>=')
        quiet: true

    - name: Get information on installed packages
      package_facts:
        manager: rpm

    - name: Ensure repositories that provide leapp utility are enabled
      community.general.ini_file:
        path: "{{ item.repo_file }}"
        section: "{{ item.name }}"
        option: enabled
        value: 1
        mode: u=rw,go=r
        no_extra_spaces: true
      loop: "{{ leapp_dependencies.repos[provider] }}"

    - name: Install packages that provide the leapp utility
      yum:
        name: "{{ leapp_dependencies.packages[provider] }}"
        state: present

    - name: Copy leapp metadata archive
      copy:
        src: "{{ leapp_archive }}"
        dest: /etc/leapp/files/{{ leapp_archive | basename }}
        mode: preserve

    - name: Unarchive leapp metadata
      unarchive:
        src: /etc/leapp/files/{{ leapp_archive | basename }}
        dest: /etc/leapp/files/
        remote_src: true

    # Address upgrade inhibitors
    # PermitRootLogin option is required to be defined explicitly
    # PermitRootLogin defaults to 'yes' but 'no' is more secure so it's preferred
    - name: Ensure PermitRootLogin is defined
      block:
        - name: Get sshd settings
          command: sshd -T
          changed_when: false
          register: pre_upgrade_sshd_settings

        - name: Check if PermitRootLogin is defined
          command: grep -i '^PermitRootLogin' /etc/ssh/sshd_config
          changed_when: false
          register: grep_permit_root_login
          failed_when: grep_permit_root_login.rc > 1

        - name: Define PermitRootLogin
          when:
            - "'permitrootlogin no' not in pre_upgrade_sshd_settings.stdout_lines"
            - grep_permit_root_login.rc == 1
          lineinfile:
            dest: /etc/ssh/sshd_config
            regexp: '^#PermitRootLogin'
            line: 'PermitRootLogin no'
            state: present
            backup: true
          register: sshd_config

    - &RESTART_SSHD
      name: Restart sshd service
      when: sshd_config.changed
      systemd:
        name: sshd
        state: restarted

    - name: Disable kernel modules that are not supported in RHEL 8
      modprobe:
        name: "{{ item }}"
        state: absent
      loop:
        - floppy
        - pata_acpi

    - name: Keep only one kernel-devel package when multiple are present
      block:
        - name: Get list of all installed packages
          yum:  # we need envra format thus yum module is used
            list: installed
          register: installed_packages

        - name: Register installed versions of kernel-devel package
          set_fact:
            installed_kernel_devel_packages: "{{ installed_packages | json_query(_query) }}"
          vars:
            _query: results[?name=='kernel-devel'].envra

        - name: Remove old versions of kernel-devel package
          when: installed_kernel_devel_packages | count > 1
          yum:
            name: "{{ installed_kernel_devel_packages[:-1] }}"  # keep the last item
            state: absent

    - name: Set PostgreSQL version
      set_fact:
        postgresql_version: >-
          {{ '10' if ansible_facts.packages['postgresql10-server'] is defined else
             '13' if ansible_facts.packages['postgresql13-server'] is defined else
             'null' }}

    - name: Add PostgreSQL repository
      when: postgresql_version != 'null'
      yum_repository:
        name: pgdg{{ postgresql_version }}
        file: pgdg{{ postgresql_version }}-rhel-{{ versions.target.full }}
        description: PostgreSQL {{ postgresql_version }} for RHEL/CentOS {{ versions.target.full }} - $basearch
        baseurl: https://download.postgresql.org/pub/repos/yum/{{ postgresql_version }}/redhat/rhel-{{ versions.target.full }}-$basearch
        gpgkey: https://download.postgresql.org/pub/repos/yum/keys/PGDG-RPM-GPG-KEY-RHEL
        enabled: false
        gpgcheck: true
        module_hotfixes: true
        exclude:  # prevent auto-upgrade from 4.0.6-1.el7
          - repmgr10
          - repmgr_10

    ### UPGRADE ###

    - name: Provide leapp answer about pam_pkcs11_module removal
      command: leapp answer --add --section remove_pam_pkcs11_module_check.confirm=True

    - name: Start leapp upgrade
      command: >-
        leapp upgrade --target {{ versions.target.full }} {{ '--no-rhsm' if provider != 'non_cloud' }}
        {{ '--enablerepo pgdg' ~ postgresql_version if postgresql_version != 'null' }}

    - name: Reboot system to complete leapp upgrade procedure
      reboot:
        msg: Reboot initiated by Ansible due to major release upgrade
        connect_timeout: "{{ reboot.connect_timeout }}"
        reboot_timeout: "{{ reboot.upgrade_timeout }}"
      vars:
        # disable connection sharing to avoid random warning on stderr: Failed to reset connection
        ansible_control_path: none

    # Upgrade removes python2 but Ansible wants to use it after the reboot.
    # Refreshing Ansible facts does not solve this issue.
    - name: Switch ansible_python_interpreter
      set_fact:
        ansible_python_interpreter: /usr/libexec/platform-python

    # leapp_resume is temporary service which resumes execution after reboot (it is removed automatically)
    - name: Wait until /etc/systemd/system/leapp_resume.service is removed
      wait_for:
        path: /etc/systemd/system/leapp_resume.service
        state: absent

    ### POST-UPGRADE -- VERIFICATION ###

    - *REFRESH_ANSIBLE_FACTS

    - name: Assert target version
      assert:
        that:
          - ansible_distribution_version is version(versions.target.full, '=')
          - ansible_kernel is version('4.18.0-305', '>=')
        quiet: true

    - name: Verify subscription status for non_cloud machines
      when: provider == 'non_cloud'
      block:
        - name: Register OS version in subscription-maganer
          shell: "subscription-manager list --installed | awk '/Version:/ { print $2 }'"
          register: subscription_version

        - name: Register subscription status
          shell: "subscription-manager list --installed | awk '/Status:/ { print $2 }'"
          register: subscription_status

        - name: Check that upgraded version remains correctly subscribed
          assert:
            that:
              - subscription_version.stdout == versions.target.full
              - subscription_status.stdout == "Subscribed"
            quiet: true

    ### POST-UPGRADE -- CLEANUP ###

    - name: Remove packages from the dnf exclude list  # populated by leapp during upgrade
      community.general.ini_file:
        path: /etc/dnf/dnf.conf
        section: main
        option: exclude
        value: ''
        mode: u=rw,go=r
        no_extra_spaces: true

    ## Remove Leapp

    - name: Remove leapp packages
      package:
        name: 'leapp*'  # Leapp packages may have different nomenclature in RHEL 8
        state: absent

    - name: Remove leapp configuration
      file:
        path: /etc/leapp
        state: absent

    - name: Remove PostgreSQL repository
      when: postgresql_version != 'null'
      yum_repository:
        name: pgdg{{ postgresql_version }}
        file: pgdg{{ postgresql_version }}-rhel-{{ versions.target.full }}
        state: absent

    ## Remove RHEL 7 packages

    - name: Remove remaining RHEL 7 packages
      block:
        - name: Gather list of remaining RHEL 7 packages
          shell: rpm -qa | grep -e '\\.el7' | grep -vE '^(gpg-pubkey|libmodulemd|katello-ca-consumer)'
          register: remaining_rhel7_packages
          changed_when: false
          failed_when: remaining_rhel7_packages.rc > 1

        - name: Remove remaining RHEL 7 packages
          when: remaining_rhel7_packages.stdout_lines | length > 0
          dnf:
            name: "{{ remaining_rhel7_packages.stdout_lines }}"
            state: absent

        - name: Determine RHEL 7 old kernel versions
          command: ls -1d *.el7*
          args:
            chdir: /lib/modules
          register: old_kernel_versions
          changed_when: false
          failed_when: old_kernel_versions.rc not in [0, 2]  # 2 when nothing found

        - name: Remove weak modules from old kernels
          command: /usr/sbin/weak-modules --remove-kernel {{ item }}
          loop: "{{ old_kernel_versions.stdout_lines }}"

        - name: Remove old kernels from bootloader entries
          command: /bin/kernel-install remove {{ item }} /lib/modules/{{ item }}/vmlinuz
          loop: "{{ old_kernel_versions.stdout_lines }}"

    - name: Update EPEL repo
      when:
        - provider == 'non_cloud'
        - "'Extra Packages for Enterprise Linux 7' in pre_upgrade_enabled_repositories.stdout"
      block:
        - name: Remove EPEL 7 repository
          package:
            name: epel-release
            state: absent

        - name: Install GPG key for EPEL 8 package
          rpm_key:
            key: https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-8

        - name: Install EPEL 8 repository
          yum:
            name: https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm
            state: present

    ## Fix failed services

    - name: Gather service facts
      service_facts: ~

    - &SET_FAILED_SERVICES_FACT
      name: Set list of failed services
      set_fact:
        failed_services: "{{ ansible_facts.services | json_query('*[] | [?(@.status==`failed`)].name') }}"

    - name: Print failed services
      when: failed_services | count > 0
      debug:
        var: failed_services

    - name: Fix repmgr service
      when: failed_services | select('match', 'repmgr[0-9]{2}\.service')
      block:
        - name: Set repmgr service name
          set_fact:
            repmgr_service_name: >-
              {{ failed_services | select('match', 'repmgr[0-9]{2}\.service') | first }}

        # upstream node must be running before repmgrd can start
        - name: Search for PostgreSQL primary node
          become_user: postgres
          # command prints primary/standby
          shell: |-
            set -o pipefail && \
            repmgr node status | grep -ioP '(?<=Role:).+' | xargs
          changed_when: false
          register: pg_node_role
          failed_when: pg_node_role.rc != 0 or pg_node_role.stdout == ""

        - name: Wait for PostgreSQL primary node to be reachable
          when: pg_node_role.stdout == 'primary'
          wait_for:
            port: 5432
            timeout: 30

        - name: Restart repmgr service
          when: pg_node_role.stdout == 'standby'
          systemd:
            name: "{{ repmgr_service_name }}"
            state: restarted

    - name: Fix filebeat service
      when: "'filebeat.service' in failed_services"
      block:
        - name: Wait for Kibana port
          when: groups.kibana[0] is defined
          delegate_to: "{{ groups.kibana[0] }}"
          wait_for:
            host: "{{ hostvars[groups.kibana.0].ansible_default_ipv4.address }}"
            port: 5601
            timeout: 30

        - name: Restart filebeat service
          systemd:
            name: filebeat
            state: restarted

    - name: Azure specific block
      when: provider == 'azure'
      block:
        - name: Fix cloud-init.service
          when: "'cloud-init.service' in failed_services"
          block:
            - name: Replace old cloud-init config file
              copy:
                remote_src: true
                src: /etc/cloud/cloud.cfg.rpmnew
                dest: /etc/cloud/cloud.cfg
                backup: true
                force: true
                mode: preserve

            - name: Disable ssh hostkeys deletion  # avoids regeneration of hostkeys after removing cache on service restart
              lineinfile:
                dest: /etc/cloud/cloud.cfg
                regexp: '^ssh_deletekeys: 1'
                line: 'ssh_deletekeys: 0'
                state: present
              register: cloud_init_cfg_ssh_deletekeys

            - name: Remove cloud-init cache  # initially populated with python2 dependencies that are unmet in RHEL 8
              block:
                - name: Find files holding the cloud-init service cache
                  find:
                    paths: /var/lib/cloud/instances
                    patterns: 'obj.pkl'
                    recurse: true
                  register: cloud_init_cache

                - name: Remove cloud-init cache files
                  file:
                    path: "{{ item }}"
                    state: absent
                  loop: "{{ cloud_init_cache.files | map(attribute='path') }}"

            - name: Restart cloud-init service  # Azure /mnt mount point is cloud-init dependent
              systemd:
                name: cloud-init
                state: restarted
              # On K8s master with Calico CNI plugin there is error in first attempt:
              # duplicate mac found! both 'cali770930d50fa' and 'cali67622b483b3' have mac 'ee:ee:ee:ee:ee:ee'
              register: restart_cloud_init
              until: restart_cloud_init is succeeded
              retries: 1
              delay: 1

            - name: Restore cloud-init config file
              when: cloud_init_cfg_ssh_deletekeys.changed
              copy:
                remote_src: true
                src: /etc/cloud/cloud.cfg.rpmnew
                dest: /etc/cloud/cloud.cfg
                force: true
                mode: preserve

        - name: Fix temp-disk-dataloss-warning.service
          when: "'temp-disk-dataloss-warning.service' in failed_services"
          block:
            - name: Remount Azure specific cloud-init filesystem  # /dev/disk/cloud/azure_resource-part1
              command: mount -av  # remount all filesystems from /etc/fstab except partitions with noauto option
              register: result
              changed_when: "'successfully mounted' in result.stdout"

            - name: Restart temp-disk-dataloss-warning service
              systemd:
                name: temp-disk-dataloss-warning
                state: restarted

        - name: Fix waagent-network-setup.service
          when: "'waagent-network-setup.service' in failed_services"
          block:
            # This file is auto-generated by WALinuxAgent, after upgrade it contains non-existing path to python2
            - name: Remove old waagent-network-setup.service unit file
              file:
                path: /usr/lib/systemd/system/waagent-network-setup.service
                state: absent

            - name: Restart waagent service  # will re-create waagent-network-setup.service unit
              systemd:
                name: waagent
                state: restarted

            - name: Wait for new waagent-network-setup.service unit file
              wait_for:
                path: /usr/lib/systemd/system/waagent-network-setup.service
                state: present

            - name: Restart waagent-network-setup service
              systemd:
                name: waagent-network-setup
                state: restarted
                daemon_reload: true

    ## Verify services

    - name: Refresh service facts
      when: failed_services | count > 0
      service_facts: ~

    - *SET_FAILED_SERVICES_FACT

    # user@<UID>.service may fail due to bug that is fixed in RHEL 8.5 (https://access.redhat.com/solutions/5931241)
    - name: Assert there are no failed services
      assert:
        that: failed_services | reject('match', 'user@') | count == 0
        fail_msg: "Failed service(s) found: {{ failed_services | join(', ') }}"
        quiet: true

    ### POST-UPGRADE -- UPDATE CONFIGURATION ###

    - name: Re-enable SElinux
      when: pre_upgrade_selinux_facts.status == 'enabled'
      block:
        - name: Print pre-upgrade SElinux settings
          debug:
            var: pre_upgrade_selinux_facts

        - name: Re-enable SELinux
          ansible.posix.selinux:
            policy: "{{ pre_upgrade_selinux_facts.type }}"
            state: "{{ pre_upgrade_selinux_facts.config_mode }}"
          register: restore_selinux

    - name: Unset subscription-manager release preference
      when: provider == 'non_cloud'
      block:
        - name: Check if release preference is set
          command: subscription-manager release --show
          register: subscription_manager_release
          changed_when: false

        - name: Unset subscription-manager release preference to consume the latest RHEL content
          when: "'Release not set' not in subscription_manager_release.stdout"
          command: subscription-manager release --unset
          register: result
          failed_when: result.rc > 1 # May return code 1 even when correctly subscribed if system purpose is not defined

    # download-requirements.py fails if releasever = 8.4 (2ndQuadrant repo)
    - name: Remove releasever DNF variable
      file:
        path: /etc/dnf/vars/releasever  # file created by upgrade
        state: absent

    # AWS: Resume HealthCheck process

    - name: Resume HealthCheck process for auto scaling groups
      when: provider == 'aws'
      run_once: true
      delegate_to: localhost
      block:
        - name: Check if file with original configuration of ASGs exists
          stat:
            path: "{{ asg_config_file_path }}"
            get_attributes: false
            get_checksum: false
            get_mime: false
          register: stat_asg_config_yml

        - name: Restore original configuration except for ReplaceUnhealthy process
          when: stat_asg_config_yml.stat.exists
          block:
            - name: Load original configuration from backup
              slurp:
                src: "{{ asg_config_file_path }}"
              register: slurp_asg_config_yml

            - name: Set ASG settings to restore
              set_fact:
                asgs_to_restore: "{{ slurp_asg_config_yml['content'] | b64decode | from_yaml }}"

            - name: Resume HealthCheck process
              community.aws.ec2_asg:
                name: "{{ item.auto_scaling_group_name }}"
                suspend_processes: "{{ item.suspended_processes | union(['ReplaceUnhealthy']) }}"
                region: "{{ aws_region }}"
              loop_control:
                label: "{{ item.auto_scaling_group_name }}"
              loop: "{{ asgs_to_restore }}"

            - name: Remove backup of original configuration of ASGs
              file:
                path: "{{ asg_config_file_path }}"
                state: absent

        - name: Remove AWS credentials file
          file:
            path: "{{ aws_config_dir }}/credentials"
            state: absent

        - name: Restore AWS credentials file
          vars:
            _backup_path: "{{ aws_config_dir }}/credentials.rhel-7-upgrade.bak"
          block:
            - name: Check if backup of AWS credentials file exists
              stat:
                path: "{{ _backup_path }}"
                get_attributes: false
                get_checksum: false
                get_mime: false
              register: stat_aws_credentials_file_backup

            - name: Restore AWS credentials file
              when: stat_aws_credentials_file_backup.stat.exists
              copy:
                src: "{{ _backup_path }}"
                dest: "{{ aws_config_dir }}/credentials"
                remote_src: true
                mode: preserve
              no_log: true

            - name: Remove backup of AWS credentials file
              when: stat_aws_credentials_file_backup.stat.exists
              file:
                path: "{{ _backup_path }}"
                state: absent

        - name: Uninstall AWS cli
          when: install_awscli.changed
          pip:
            name: awscli
            state: absent
